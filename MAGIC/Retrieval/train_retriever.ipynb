{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu_109550068/.conda/envs/wy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.document_stores import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "passage_model = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "\n",
    "save_dir = \"./saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu_109550068/.conda/envs/wy/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Preprocessing dataset: 100%|██████████| 8/8 [00:08<00:00,  1.05s/ Dicts]\n",
      "Preprocessing dataset: 100%|██████████| 3/3 [00:02<00:00,  1.04 Dicts/s]\n",
      "Train epoch 0/4 (Cur. train loss: 4.8421): 100%|██████████| 58/58 [00:38<00:00,  1.50it/s] \n",
      "Train epoch 1/4 (Cur. train loss: 1.7956): 100%|██████████| 58/58 [00:38<00:00,  1.50it/s]\n",
      "Train epoch 2/4 (Cur. train loss: 1.6109): 100%|██████████| 58/58 [00:38<00:00,  1.49it/s]\n",
      "Train epoch 3/4 (Cur. train loss: 2.0401): 100%|██████████| 58/58 [00:38<00:00,  1.49it/s]\n",
      "Train epoch 4/4 (Cur. train loss: 1.0595): 100%|██████████| 58/58 [00:38<00:00,  1.49it/s]\n",
      "Preprocessing dataset: 100%|██████████| 4/4 [00:04<00:00,  1.06s/ Dicts]\n",
      "Preprocessing dataset: 100%|██████████| 2/2 [00:01<00:00,  1.43 Dicts/s]\n",
      "Train epoch 0/4 (Cur. train loss: 7.4010): 100%|██████████| 30/30 [00:19<00:00,  1.52it/s] \n",
      "Train epoch 1/4 (Cur. train loss: 3.4411): 100%|██████████| 30/30 [00:19<00:00,  1.51it/s]\n",
      "Train epoch 2/4 (Cur. train loss: 3.0282): 100%|██████████| 30/30 [00:19<00:00,  1.51it/s]\n",
      "Train epoch 3/4 (Cur. train loss: 1.4941): 100%|██████████| 30/30 [00:19<00:00,  1.51it/s]\n",
      "Train epoch 4/4 (Cur. train loss: 2.2002): 100%|██████████| 30/30 [00:19<00:00,  1.51it/s]\n",
      "Preprocessing dataset: 100%|██████████| 2/2 [00:02<00:00,  1.21s/ Dicts]\n",
      "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  1.23 Dicts/s]\n",
      "Train epoch 0/4 (Cur. train loss: 9.7191): 100%|██████████| 16/16 [00:10<00:00,  1.55it/s] \n",
      "Train epoch 1/4 (Cur. train loss: 6.2597): 100%|██████████| 16/16 [00:10<00:00,  1.55it/s] \n",
      "Train epoch 2/4 (Cur. train loss: 4.5840): 100%|██████████| 16/16 [00:10<00:00,  1.55it/s]\n",
      "Train epoch 3/4 (Cur. train loss: 2.2083): 100%|██████████| 16/16 [00:10<00:00,  1.55it/s]\n",
      "Train epoch 4/4 (Cur. train loss: 3.2613): 100%|██████████| 16/16 [00:10<00:00,  1.54it/s]\n",
      "Preprocessing dataset: 100%|██████████| 3/3 [00:02<00:00,  1.10 Dicts/s]\n",
      "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  1.09 Dicts/s]\n",
      "Train epoch 0/4 (Cur. train loss: 7.0586): 100%|██████████| 19/19 [00:12<00:00,  1.54it/s] \n",
      "Train epoch 1/4 (Cur. train loss: 3.5972): 100%|██████████| 19/19 [00:12<00:00,  1.54it/s]\n",
      "Train epoch 2/4 (Cur. train loss: 4.2581): 100%|██████████| 19/19 [00:12<00:00,  1.54it/s]\n",
      "Train epoch 3/4 (Cur. train loss: 3.8894): 100%|██████████| 19/19 [00:12<00:00,  1.54it/s]\n",
      "Train epoch 4/4 (Cur. train loss: 2.0613): 100%|██████████| 19/19 [00:12<00:00,  1.54it/s]\n",
      "Preprocessing dataset: 100%|██████████| 4/4 [00:05<00:00,  1.25s/ Dicts]\n",
      "Preprocessing dataset: 100%|██████████| 2/2 [00:01<00:00,  1.24 Dicts/s]\n",
      "Train epoch 0/4 (Cur. train loss: 4.2310): 100%|██████████| 32/32 [00:21<00:00,  1.52it/s] \n",
      "Train epoch 1/4 (Cur. train loss: 4.2395): 100%|██████████| 32/32 [00:21<00:00,  1.52it/s]\n",
      "Train epoch 2/4 (Cur. train loss: 1.8010): 100%|██████████| 32/32 [00:21<00:00,  1.52it/s]\n",
      "Train epoch 3/4 (Cur. train loss: 2.9823): 100%|██████████| 32/32 [00:21<00:00,  1.51it/s]\n",
      "Train epoch 4/4 (Cur. train loss: 1.2875): 100%|██████████| 32/32 [00:21<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for i in range(5):\n",
    "    with torch.cuda.device('cuda'):\n",
    "        retriever = DensePassageRetriever(\n",
    "            document_store=InMemoryDocumentStore(),\n",
    "            query_embedding_model=query_model,\n",
    "            passage_embedding_model=passage_model,\n",
    "            max_seq_len_query=64,\n",
    "            max_seq_len_passage=128,\n",
    "        )\n",
    "        retriever.train(\n",
    "            data_dir='./dataset/',\n",
    "            train_filename=f'train/{i+1}.json',\n",
    "            # dev_filename=f'valid/{i+1}.json',\n",
    "            n_epochs=5,\n",
    "            batch_size=64,\n",
    "            grad_acc_steps=2,\n",
    "            save_dir=f'{save_dir}/{i+1}/',\n",
    "            evaluate_every=2000,\n",
    "            embed_title=False,\n",
    "            num_positives=1,\n",
    "            num_hard_negatives=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude of the vectorized matrix: 10.247812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectorized_matrix = output.flatten()\n",
    "\n",
    "magnitude = np.linalg.norm(vectorized_matrix)\n",
    "\n",
    "print(\"Magnitude of the vectorized matrix:\", magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999\n"
     ]
    }
   ],
   "source": [
    "output /= magnitude\n",
    "print(np.dot(output[0], output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
