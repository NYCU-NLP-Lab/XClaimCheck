{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../retriever/dataset/split.json', 'r') as file:\n",
    "    split = json.load(file)\n",
    "with open('../topics.json', 'r') as file:\n",
    "    topics = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {'false': 0, 'on_fire': 0, 'mostly_false': 1, 'half_true': 2, 'mostly_true': 3, 'true': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy\n",
      "Jobs\n",
      "Legal Issues\n",
      "Economy\n",
      "Taxes\n",
      "Technology\n",
      "Education\n",
      "Health Care\n",
      "Immigration\n",
      "Foreign Policy\n",
      "LGBTQ\n",
      "Military\n",
      "Elections\n",
      "Environment\n",
      "Transportation\n",
      "Terrorism\n",
      "Sports\n",
      "State Budget\n",
      "Federal Budget\n",
      "Criminal Justice\n",
      "Religion\n",
      "Government Regulation\n",
      "Candidate Biography\n",
      "Homeland Security\n",
      "Social Security\n",
      "History\n",
      "Group 5 - In group: 220/664, Out group: 763/2560\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    in_cnt = 0\n",
    "    in_sum = 0\n",
    "    out_cnt = 0\n",
    "    out_sum = 0\n",
    "    output = {}\n",
    "    path = Path(f'./model/{i+1}')\n",
    "    sub = path.glob('*')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(sorted(sub)[-1])\n",
    "    for topic in topics.keys():\n",
    "        print(topic)\n",
    "        for label in ref.keys():\n",
    "            hit = 0\n",
    "            total = 0\n",
    "            path = Path(f'../retrieval/{i+1}/test/{topic}/{label}')\n",
    "            sub = path.glob('*')\n",
    "            num = len(list(sub))\n",
    "            for j in range(num):\n",
    "                idx = j+1\n",
    "                path = Path(f'../retrieval/{i+1}/test/{topic}/{label}/{idx}')\n",
    "                sub = path.glob('*')\n",
    "                with open(f'../dataset/test/{topic}/{label}/{idx}/info.json', 'r') as file:\n",
    "                    info = json.load(file)\n",
    "                claim = info['claim']\n",
    "                cnt = {'false': 0, 'mostly_false': 0, 'half_true': 0, 'mostly_true': 0, 'true': 0}\n",
    "                for s in sub:\n",
    "                    with open(s, 'r') as file:\n",
    "                        evidence = json.load(file)\n",
    "                    text = \"\"\n",
    "                    for e in evidence:\n",
    "                        text += str(e) + ' '\n",
    "                    input = f'Claim: {claim}\\nReference: {text}'\n",
    "                    inputs = tokenizer(input, return_tensors=\"pt\", truncation=True)\n",
    "                    with torch.no_grad():\n",
    "                        logits = model(**inputs).logits\n",
    "                    predicted_class_id = logits.argmax().item()\n",
    "                    score = logits[0][predicted_class_id]\n",
    "                    predicted_class = model.config.id2label[predicted_class_id]\n",
    "                    if score >= 0.25:\n",
    "                        cnt[predicted_class] += 1\n",
    "                sorted_cnt = dict(sorted(cnt.items(), key=lambda item: item[1], reverse=True))\n",
    "                prediction = list(sorted_cnt.keys())[0]\n",
    "                if label == 'on_fire':\n",
    "                    if prediction == 'false':\n",
    "                        prediction = 'on_fire'\n",
    "                total += 1\n",
    "                if prediction == label:\n",
    "                    hit += 1\n",
    "                if topic in split[i]:\n",
    "                    in_sum += 1\n",
    "                    if prediction == label:\n",
    "                        in_cnt += 1\n",
    "                else:\n",
    "                    out_sum += 1\n",
    "                    if prediction == label:\n",
    "                        out_cnt += 1\n",
    "            \n",
    "            out = {'hit': hit, 'total': total, 'rate': hit/total}\n",
    "            if topic not in output:\n",
    "                output[topic] = {}\n",
    "            output[topic][label] = out\n",
    "    with open(f'../retrieve-baseline-{i+1}.json', 'w') as file:\n",
    "        json.dump(output, file)\n",
    "    # print(f'Group {i+1} - In group: {in_cnt}/{in_sum}, Out group: {out_cnt}/{out_sum}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
